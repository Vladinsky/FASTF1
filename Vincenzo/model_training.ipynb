{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Tire Change Prediction - Training Pipeline\n",
    "## üèéÔ∏è Formula 1 Multi-task RNN Model\n",
    "\n",
    "**Obiettivo**: Predire cambi gomme utilizzando sequenze temporali di telemetria F1\n",
    "\n",
    "**Architettura**:\n",
    "- LSTM multi-task (cambio gomme + tipo mescola)\n",
    "- Gestione class imbalance (1:29 ratio)\n",
    "- Threshold optimization per target recall ‚â•80%\n",
    "- CPU/GPU adaptive training\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup e Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Models package (nostro)\n",
    "from dataset.models import (\n",
    "    LSTMTireChangePredictor,\n",
    "    create_cpu_optimized_loaders,\n",
    "    create_data_loaders,\n",
    "    create_trainer,\n",
    "    ModelEvaluator,\n",
    "    create_evaluation_report,\n",
    "    quick_setup_cpu,\n",
    "    quick_setup_gpu,\n",
    "    get_model_summary\n",
    ")\n",
    "\n",
    "# Configurazione plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ Imports completed successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurazione principale\n",
    "CONFIG = {\n",
    "    # Paths\n",
    "    'data_dir': './dataset/preprocessed',\n",
    "    'output_dir': './training_outputs',\n",
    "    'logs_dir': './logs',\n",
    "    'checkpoints_dir': './checkpoints',\n",
    "    \n",
    "    # Device configuration\n",
    "    'device': 'auto',  # 'auto', 'cpu', 'cuda'\n",
    "    'use_cpu_optimized': True,  # True per test rapidi, False per training completo\n",
    "    \n",
    "    # Training parameters\n",
    "    'batch_size': 16,  # 16 per CPU, 64 per GPU\n",
    "    'learning_rate': 0.001,\n",
    "    'num_epochs': 20,  # 5 per test rapido, 50+ per training completo\n",
    "    'early_stopping_patience': 8,\n",
    "    'target_recall': 0.8,\n",
    "    \n",
    "    # Model parameters (CPU-friendly)\n",
    "    'hidden_size': 32,  # 32 per CPU, 128 per GPU\n",
    "    'num_layers': 2,    # 2 per CPU, 3 per GPU\n",
    "    'dropout': 0.2,\n",
    "    \n",
    "    # Data augmentation\n",
    "    'augment_positive': True,\n",
    "    'augment_factor': 3,\n",
    "    \n",
    "    # Logging\n",
    "    'verbose': True,\n",
    "    'save_plots': True,\n",
    "    'tensorboard': True\n",
    "}\n",
    "\n",
    "# Auto-detect device\n",
    "if CONFIG['device'] == 'auto':\n",
    "    if torch.cuda.is_available():\n",
    "        CONFIG['device'] = 'cuda'\n",
    "        CONFIG['batch_size'] = 64\n",
    "        CONFIG['hidden_size'] = 128\n",
    "        CONFIG['num_layers'] = 3\n",
    "        CONFIG['use_cpu_optimized'] = False\n",
    "        print(\"üöÄ GPU detected - Using GPU-optimized configuration\")\n",
    "    else:\n",
    "        CONFIG['device'] = 'cpu'\n",
    "        print(\"üíª Using CPU-optimized configuration\")\n",
    "\n",
    "print(f\"Device: {CONFIG['device']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Hidden size: {CONFIG['hidden_size']}\")\n",
    "print(f\"Epochs: {CONFIG['num_epochs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Data Loading e Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica dati preprocessati\n",
    "data_dir = Path(CONFIG['data_dir'])\n",
    "required_files = [\n",
    "    'X_train.npy', 'X_val.npy', 'X_test.npy',\n",
    "    'y_change_train.npy', 'y_change_val.npy', 'y_change_test.npy',\n",
    "    'y_type_train.npy', 'y_type_val.npy', 'y_type_test.npy'\n",
    "]\n",
    "\n",
    "print(\"üîç Checking preprocessed data...\")\n",
    "missing_files = []\n",
    "for file in required_files:\n",
    "    file_path = data_dir / file\n",
    "    if file_path.exists():\n",
    "        file_size = file_path.stat().st_size / (1024 * 1024)  # MB\n",
    "        print(f\"   ‚úÖ {file} ({file_size:.1f} MB)\")\n",
    "    else:\n",
    "        missing_files.append(file)\n",
    "        print(f\"   ‚ùå {file} - MISSING\")\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\n‚ö†Ô∏è Missing {len(missing_files)} files. Run data_preprocessing.py first!\")\n",
    "    raise FileNotFoundError(f\"Missing files: {missing_files}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All data files found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica DataLoaders\n",
    "print(\"üì¶ Loading DataLoaders...\")\n",
    "\n",
    "if CONFIG['use_cpu_optimized']:\n",
    "    data_loaders = create_cpu_optimized_loaders(str(data_dir))\n",
    "else:\n",
    "    data_loaders = create_data_loaders(\n",
    "        data_dir=str(data_dir),\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        num_workers=2 if CONFIG['device'] == 'cpu' else 4,\n",
    "        pin_memory=CONFIG['device'] != 'cpu',\n",
    "        augment_positive=CONFIG['augment_positive'],\n",
    "        device=CONFIG['device']\n",
    "    )\n",
    "\n",
    "print(f\"‚úÖ DataLoaders created:\")\n",
    "print(f\"   Train: {len(data_loaders['train'])} batches\")\n",
    "print(f\"   Val: {len(data_loaders['val'])} batches\")\n",
    "print(f\"   Test: {len(data_loaders['test'])} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test un batch per validazione shape\n",
    "print(\"üß™ Testing data batch...\")\n",
    "\n",
    "train_loader = data_loaders['train']\n",
    "sequences, tire_change_targets, tire_type_targets = next(iter(train_loader))\n",
    "\n",
    "print(f\"Batch shapes:\")\n",
    "print(f\"   Sequences: {sequences.shape}\")\n",
    "print(f\"   Tire change targets: {tire_change_targets.shape}\")\n",
    "print(f\"   Tire type targets: {tire_type_targets.shape}\")\n",
    "\n",
    "print(f\"\\nData types:\")\n",
    "print(f\"   Sequences: {sequences.dtype}\")\n",
    "print(f\"   Tire change targets: {tire_change_targets.dtype}\")\n",
    "print(f\"   Tire type targets: {tire_type_targets.dtype}\")\n",
    "\n",
    "print(f\"\\nSample targets:\")\n",
    "print(f\"   Tire changes: {tire_change_targets[:5].tolist()}\")\n",
    "print(f\"   Tire types: {tire_type_targets[:5].tolist()}\")\n",
    "\n",
    "# Check for any NaN values\n",
    "has_nan = torch.isnan(sequences).any()\n",
    "print(f\"\\nData quality:\")\n",
    "print(f\"   Has NaN values: {has_nan}\")\n",
    "print(f\"   Sequences range: [{sequences.min():.3f}, {sequences.max():.3f}]\")\n",
    "\n",
    "print(\"‚úÖ Data validation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea modello\n",
    "print(\"üèóÔ∏è Creating LSTM model...\")\n",
    "\n",
    "input_size = sequences.shape[-1]  # Numero features (52)\n",
    "\n",
    "model = LSTMTireChangePredictor(\n",
    "    input_size=input_size,\n",
    "    hidden_size=CONFIG['hidden_size'],\n",
    "    num_layers=CONFIG['num_layers'],\n",
    "    num_compounds=7,  # SOFT, MEDIUM, HARD, etc.\n",
    "    dropout=CONFIG['dropout'],\n",
    "    device=CONFIG['device']\n",
    ")\n",
    "\n",
    "# Move to device\n",
    "model = model.to(CONFIG['device'])\n",
    "\n",
    "# Model summary\n",
    "summary = model.get_model_summary()\n",
    "print(f\"\\nüìã Model Summary:\")\n",
    "print(f\"   Architecture: {summary['architecture']}\")\n",
    "print(f\"   Input size: {summary['input_size']}\")\n",
    "print(f\"   Hidden size: {summary['hidden_size']}\")\n",
    "print(f\"   Layers: {summary['num_layers']}\")\n",
    "print(f\"   Total parameters: {summary['total_parameters']:,}\")\n",
    "print(f\"   Trainable parameters: {summary['trainable_parameters']:,}\")\n",
    "print(f\"   Model size: {summary['model_size_mb']:.2f} MB\")\n",
    "print(f\"   Device: {summary['device']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "print(\"üß™ Testing model forward pass...\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Test con un small batch\n",
    "    test_input = sequences[:4].to(CONFIG['device'])\n",
    "    \n",
    "    outputs = model(test_input)\n",
    "    predictions = model.predict_probabilities(test_input)\n",
    "    \n",
    "    print(f\"Forward pass shapes:\")\n",
    "    print(f\"   Tire change logits: {outputs['tire_change_logits'].shape}\")\n",
    "    print(f\"   Tire type logits: {outputs['tire_type_logits'].shape}\")\n",
    "    \n",
    "    print(f\"\\nSample predictions:\")\n",
    "    tire_change_probs = predictions['tire_change_probs'].squeeze().cpu().numpy()\n",
    "    print(f\"   Tire change probabilities: {tire_change_probs}\")\n",
    "    \n",
    "    tire_type_probs = predictions['tire_type_probs'].cpu().numpy()\n",
    "    print(f\"   Tire type prob shape: {tire_type_probs.shape}\")\n",
    "    \n",
    "model.train()  # Back to training mode\n",
    "print(\"‚úÖ Model forward pass successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea trainer e scheduler\n",
    "print(\"üéØ Setting up training pipeline...\")\n",
    "\n",
    "# Crea directories per output\n",
    "output_dir = Path(CONFIG['output_dir'])\n",
    "logs_dir = Path(CONFIG['logs_dir'])\n",
    "checkpoints_dir = Path(CONFIG['checkpoints_dir'])\n",
    "\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "logs_dir.mkdir(exist_ok=True, parents=True)\n",
    "checkpoints_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Crea trainer\n",
    "trainer, scheduler = create_trainer(\n",
    "    model=model,\n",
    "    data_loaders=data_loaders,\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    device=CONFIG['device'],\n",
    "    target_recall=CONFIG['target_recall'],\n",
    "    log_dir=str(logs_dir),\n",
    "    checkpoint_dir=str(checkpoints_dir)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Training setup completed:\")\n",
    "print(f\"   Target recall: {CONFIG['target_recall']}\")\n",
    "print(f\"   Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"   Max epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"   Early stopping patience: {CONFIG['early_stopping_patience']}\")\n",
    "print(f\"   Output directory: {output_dir}\")\n",
    "\n",
    "if CONFIG['tensorboard']:\n",
    "    print(f\"\\nüìä TensorBoard logging enabled:\")\n",
    "    print(f\"   Run: tensorboard --logdir={logs_dir} --port=6006\")\n",
    "    print(f\"   URL: http://localhost:6006\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Esegui training\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    training_history = trainer.train(\n",
    "        num_epochs=CONFIG['num_epochs'],\n",
    "        early_stopping_patience=CONFIG['early_stopping_patience'],\n",
    "        scheduler=scheduler,\n",
    "        save_frequency=5\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüéâ Training completed successfully!\")\n",
    "    print(f\"   Best F1-score: {training_history['best_f1']:.4f}\")\n",
    "    print(f\"   Best threshold: {training_history['best_threshold']:.4f}\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚èπÔ∏è Training interrupted by user\")\n",
    "    training_history = {\n",
    "        'train_history': trainer.train_history,\n",
    "        'val_history': trainer.val_history,\n",
    "        'best_f1': trainer.best_f1,\n",
    "        'best_threshold': trainer.best_threshold\n",
    "    }\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training failed: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Training Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "if 'training_history' in locals() and training_history['train_history']:\n",
    "    print(\"üìà Plotting training curves...\")\n",
    "    \n",
    "    train_hist = training_history['train_history']\n",
    "    val_hist = training_history['val_history']\n",
    "    \n",
    "    epochs = range(1, len(train_hist) + 1)\n",
    "    \n",
    "    # Setup figure\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Training Progress', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Loss curves\n",
    "    train_losses = [h['loss'] for h in train_hist]\n",
    "    val_losses = [h['loss'] for h in val_hist]\n",
    "    \n",
    "    axes[0, 0].plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "    axes[0, 0].plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Loss Curves')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # F1-Score curves\n",
    "    train_f1 = [h['f1_score'] for h in train_hist]\n",
    "    val_f1 = [h['f1_score'] for h in val_hist]\n",
    "    \n",
    "    axes[0, 1].plot(epochs, train_f1, 'b-', label='Training F1', linewidth=2)\n",
    "    axes[0, 1].plot(epochs, val_f1, 'r-', label='Validation F1', linewidth=2)\n",
    "    axes[0, 1].axhline(y=training_history['best_f1'], color='g', linestyle='--', \n",
    "                      label=f'Best F1: {training_history[\"best_f1\"]:.3f}')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('F1-Score')\n",
    "    axes[0, 1].set_title('F1-Score Curves')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Precision curves\n",
    "    train_prec = [h['precision'] for h in train_hist]\n",
    "    val_prec = [h['precision'] for h in val_hist]\n",
    "    \n",
    "    axes[1, 0].plot(epochs, train_prec, 'b-', label='Training Precision', linewidth=2)\n",
    "    axes[1, 0].plot(epochs, val_prec, 'r-', label='Validation Precision', linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Precision')\n",
    "    axes[1, 0].set_title('Precision Curves')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Recall curves\n",
    "    train_recall = [h['recall'] for h in train_hist]\n",
    "    val_recall = [h['recall'] for h in val_hist]\n",
    "    \n",
    "    axes[1, 1].plot(epochs, train_recall, 'b-', label='Training Recall', linewidth=2)\n",
    "    axes[1, 1].plot(epochs, val_recall, 'r-', label='Validation Recall', linewidth=2)\n",
    "    axes[1, 1].axhline(y=CONFIG['target_recall'], color='g', linestyle='--', \n",
    "                      label=f'Target Recall: {CONFIG[\"target_recall\"]}')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Recall')\n",
    "    axes[1, 1].set_title('Recall Curves')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if CONFIG['save_plots']:\n",
    "        plot_path = output_dir / 'training_curves.png'\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"üìä Training curves saved: {plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No training history available for plotting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valuta modello su test set\n",
    "print(\"üéØ Evaluating model on test set...\")\n",
    "\n",
    "# Usa best threshold dal training\n",
    "best_threshold = training_history['best_threshold'] if 'training_history' in locals() else 0.5\n",
    "\n",
    "# Crea evaluator\n",
    "evaluator = ModelEvaluator(\n",
    "    model=model,\n",
    "    device=CONFIG['device'],\n",
    "    threshold=best_threshold\n",
    ")\n",
    "\n",
    "# Valuta test set\n",
    "test_results = evaluator.evaluate_dataset(\n",
    "    data_loaders['test'],\n",
    "    split_name=\"Test\"\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Test Set Results:\")\n",
    "metrics = test_results['metrics']\n",
    "print(f\"   Threshold: {best_threshold:.4f}\")\n",
    "print(f\"   F1-Score: {metrics['f1_score']:.4f}\")\n",
    "print(f\"   Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"   Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"   ROC-AUC: {metrics['roc_auc']:.4f}\")\n",
    "print(f\"   PR-AUC: {metrics['pr_auc']:.4f}\")\n",
    "print(f\"   Accuracy: {metrics['accuracy']:.4f}\")\n",
    "\n",
    "if 'true_positives' in metrics:\n",
    "    print(f\"\\nüéØ Confusion Matrix:\")\n",
    "    print(f\"   True Positives: {metrics['true_positives']}\")\n",
    "    print(f\"   True Negatives: {metrics['true_negatives']}\")\n",
    "    print(f\"   False Positives: {metrics['false_positives']}\")\n",
    "    print(f\"   False Negatives: {metrics['false_negatives']}\")\n",
    "    print(f\"   Specificity: {metrics['specificity']:.4f}\")\n",
    "\n",
    "# Valuta task secondario\n",
    "tire_type_metrics = test_results['tire_type_metrics']\n",
    "if 'accuracy' in tire_type_metrics:\n",
    "    print(f\"\\nüîß Secondary Task (Tire Type):\")\n",
    "    print(f\"   Accuracy: {tire_type_metrics['accuracy']:.4f}\")\n",
    "    print(f\"   Samples evaluated: {tire_type_metrics['num_samples']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot evaluation curves\n",
    "print(\"üìä Creating evaluation plots...\")\n",
    "\n",
    "evaluation_dir = output_dir / 'evaluation'\n",
    "evaluation_dir.mkdir(exist_ok=True)\n",
    "\n",
    "evaluator.plot_evaluation_curves(\n",
    "    test_results,\n",
    "    save_dir=str(evaluation_dir) if CONFIG['save_plots'] else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold sensitivity analysis\n",
    "print(\"üîç Analyzing threshold sensitivity...\")\n",
    "\n",
    "threshold_results = evaluator.analyze_threshold_sensitivity(\n",
    "    test_results['probabilities'],\n",
    "    test_results['targets']\n",
    ")\n",
    "\n",
    "optimal_threshold, optimal_f1 = evaluator.plot_threshold_analysis(\n",
    "    threshold_results,\n",
    "    target_recall=CONFIG['target_recall'],\n",
    "    save_dir=str(evaluation_dir) if CONFIG['save_plots'] else None\n",
    ")\n",
    "\n",
    "print(f\"\\nüéØ Threshold Analysis:\")\n",
    "print(f\"   Optimal threshold: {optimal_threshold:.4f}\")\n",
    "print(f\"   Optimal F1-score: {optimal_f1:.4f}\")\n",
    "print(f\"   Training threshold: {best_threshold:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
