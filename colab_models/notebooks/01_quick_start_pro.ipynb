{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üèéÔ∏è Formula 1 Tire Change Prediction - Quick Start (Colab Pro)\n",
        "\n",
        "**Sistema completo di predizione cambi gomme F1 con RNN multi-task**\n",
        "\n",
        "Questo notebook ti guida attraverso il setup completo e il training del modello ottimizzato per Google Colab Pro.\n",
        "\n",
        "## üìã Pre-requisiti\n",
        "- Google Colab Pro (raccomandato)\n",
        "- Dati F1 caricati su Google Drive\n",
        "- GPU abilitata (Runtime > Change runtime type > GPU)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Step 1: Setup Ambiente\n",
        "\n",
        "Iniziamo con il setup automatico dell'ambiente ottimizzato per Colab Pro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monta Google Drive all'inizio per accesso ai dati e salvataggio modelli\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "  print(\"üìÇ Montaggio Google Drive...\")\n",
        "  drive.mount('/content/drive')\n",
        "else:\n",
        "  print(\"‚úÖ Google Drive gi√† montato.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/Vladinsky/FASTF1.git\n",
        "%cd FASTF1/colab_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clona repository (se necessario) o carica files\n",
        "# NOTA: Se stai usando questo notebook, presumiamo che i file siano gi√† caricati\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Verifica che siamo nella directory corretta\n",
        "if os.getcwd().endswith('FASTF1/colab_models'):\n",
        "    print(\"‚úÖ Gi√† nella directory colab_models!\")\n",
        "elif os.path.exists('colab_models'): # Se siamo in /content/FASTF1\n",
        "    print(\"üîÑ Cambio directory in colab_models...\")\n",
        "    %cd colab_models\n",
        "elif os.path.exists('FASTF1/colab_models'): # Se siamo in /content\n",
        "    print(\"üîÑ Cambio directory in FASTF1/colab_models...\")\n",
        "    %cd FASTF1/colab_models\n",
        "else:\n",
        "    print(\"‚ùå Directory colab_models non trovata! Assicurati che il git clone sia avvenuto correttamente.\")\n",
        "    print(f\"üìç Directory attuale: {os.getcwd()}\")\n",
        "\n",
        "print(f\"‚úÖ Directory di lavoro corrente: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Esegui setup automatico\n",
        "print(\"üöÄ Avvio setup automatico per Colab Pro...\")\n",
        "if os.path.exists('setup_colab_pro.py'):\n",
        "    %run setup_colab_pro.py\n",
        "else:\n",
        "    print(\"‚ùå File setup_colab_pro.py non trovato! Verifica la directory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Step 2: Unificazione Dati\n",
        "\n",
        "I dati F1 sono attualmente distribuiti in multiple cartelle. Li unifichiamo in un dataset completo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verifica dati disponibili su Drive\n",
        "import os\n",
        "\n",
        "# Assicurati che Drive sia montato (gi√† fatto sopra, ma un controllo non fa male)\n",
        "if not os.path.exists('/content/drive/MyDrive'):\n",
        "    print(\"‚ùå Google Drive non sembra montato correttamente. Riprova il montaggio.\")\n",
        "else:\n",
        "    print(\"‚úÖ Google Drive √® montato.\")\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/F1_Project/processed_races\"\n",
        "\n",
        "print(\"üìÅ Controllo dati disponibili:\")\n",
        "if os.path.exists(data_path):\n",
        "    try:\n",
        "        num_files = len(os.listdir(data_path))\n",
        "        print(f\"Percorso dati: {data_path} - Trovato ({num_files} files)\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Errore nel leggere la directory {data_path}: {e}\")\n",
        "else:\n",
        "    print(f\"‚ùå Percorso dati: {data_path} - NON Trovato!\")\n",
        "    print(\"‚ö†Ô∏è  Assicurati di aver caricato i dati su Drive nel percorso corretto e che Drive sia montato.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acf3eb18",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unifica tutti i dati in un dataset completo\n",
        "from data.data_unifier_complete import CompleteDataUnifier\n",
        "\n",
        "print(\"üîÑ Avvio unificazione dati...\")\n",
        "print(\"‚è±Ô∏è  Questo processo pu√≤ richiedere 10-15 minuti\")\n",
        "\n",
        "unifier = CompleteDataUnifier(config_path=\"colab_models/configs/data_config_colab.json\")\n",
        "dataset = unifier.unify_all_data()\n",
        "\n",
        "if dataset is not None and not dataset.empty:\n",
        "    print(f'‚úÖ Unificazione completata!')\n",
        "    print(f'üìä Dataset finale: {len(dataset):,} righe, {len(dataset.columns)} colonne')\n",
        "else:\n",
        "    print(f'‚ùå Unificazione fallita o dataset vuoto.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Esplorazione rapida del dataset unificato\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "if 'dataset' in locals() and dataset is not None and not dataset.empty:\n",
        "    print(\"üîç Esplorazione dataset unificato:\")\n",
        "    print(f'Shape: {dataset.shape}')\n",
        "    print(f'Anni: {sorted(dataset[\"Year\"].unique()) if \"Year\" in dataset.columns else \"N/A\"}')\n",
        "    print(f'Piloti: {dataset[\"Driver\"].nunique() if \"Driver\" in dataset.columns else \"N/A\"}')\n",
        "    print(f'GP: {dataset[\"EventName\"].nunique() if \"EventName\" in dataset.columns else \"N/A\"}')\n",
        "\n",
        "    # Visualizzazione distribuzione anni\n",
        "    if 'Year' in dataset.columns:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        dataset['Year'].value_counts().sort_index().plot(kind='bar')\n",
        "        plt.title('Distribuzione Dati per Anno')\n",
        "        plt.xlabel('Anno')\n",
        "        plt.ylabel('Numero di Record')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"‚ùå Dataset non disponibile per l'esplorazione.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèãÔ∏è Step 3: Training del Modello\n",
        "\n",
        "Avviamo il training completo del modello LSTM multi-task ottimizzato per Colab Pro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verifica GPU e memoria\n",
        "import torch\n",
        "import psutil\n",
        "\n",
        "print(\"üîß Controllo risorse sistema:\")\n",
        "\n",
        "# GPU\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f'‚úÖ GPU: {gpu_name} ({gpu_memory:.1f}GB)')\n",
        "    \n",
        "    if \"T4\" in gpu_name or \"A100\" in gpu_name or \"V100\" in gpu_name:\n",
        "        print(\"üöÄ GPU ottima per training!\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  GPU rilevata ma potrebbe essere lenta\")\n",
        "else:\n",
        "    print(\"‚ùå GPU non disponibile! Abilita GPU in Runtime > Change runtime type\")\n",
        "\n",
        "# RAM\n",
        "memory = psutil.virtual_memory()\n",
        "total_gb = memory.total / 1e9\n",
        "available_gb = memory.available / 1e9\n",
        "\n",
        "print(f'‚úÖ RAM: {total_gb:.1f}GB totale, {available_gb:.1f}GB disponibile')\n",
        "\n",
        "if total_gb > 20:\n",
        "    print(\"üöÄ RAM eccellente per Colab Pro!\")\n",
        "elif total_gb > 12:\n",
        "    print(\"‚úÖ RAM buona per training\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  RAM limitata, considera Colab Pro\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup trainer\n",
        "from training.train_from_scratch_pro import ProTrainer\n",
        "\n",
        "print(\"üèãÔ∏è Inizializzazione trainer...\")\n",
        "\n",
        "# Crea trainer con configurazione ottimizzata\n",
        "# Assicurati che il project_dir sia corretto e accessibile da Colab (su Drive)\n",
        "project_drive_dir = \"/content/drive/MyDrive/F1_TireChange_Project\"\n",
        "if not os.path.exists(project_drive_dir):\n",
        "    print(f\"‚ö†Ô∏è Directory progetto {project_drive_dir} non trovata su Drive. Verr√† creata.\")\n",
        "    os.makedirs(project_drive_dir, exist_ok=True)\n",
        "    os.makedirs(os.path.join(project_drive_dir, 'models/checkpoints'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(project_drive_dir, 'results'), exist_ok=True)\n",
        "    print(f\"‚úÖ Directory progetto {project_drive_dir} creata.\")\n",
        "\n",
        "trainer = ProTrainer(\n",
        "    config_path=\"configs/model_config_pro.yaml\",\n",
        "    project_dir=project_drive_dir\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer inizializzato!\")\n",
        "print(f'üì± Device: {trainer.device}')\n",
        "print(f'üìÅ Model dir: {trainer.model_dir}')\n",
        "print(f'üìä Results dir: {trainer.results_dir}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AVVIO TRAINING COMPLETO\n",
        "print(\"üöÄ AVVIO TRAINING DA ZERO...\")\n",
        "print(\"‚è±Ô∏è  Tempo stimato: 4-6 ore su Colab Pro\")\n",
        "print(\"üíæ Checkpoint automatici ogni 30 minuti\")\n",
        "print(\"üîÑ Puoi interrompere e riprendere il training\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Training con parametri ottimizzati\n",
        "if 'trainer' in locals():\n",
        "    results = trainer.train_complete(max_epochs=100)\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"üéâ TRAINING COMPLETATO!\")\n",
        "    print(f'‚è±Ô∏è  Tempo totale: {results[\"total_training_time\"]/3600:.1f} ore')\n",
        "    print(f'üéØ Best validation loss: {results[\"best_val_loss\"]:.4f}')\n",
        "else:\n",
        "    print(\"‚ùå Trainer non inizializzato. Impossibile avviare il training.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Step 4: Analisi Risultati\n",
        "\n",
        "Analizziamo i risultati del training e le performance del modello."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizza curve di training\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "if 'results' in locals() and results:\n",
        "    history = results['training_history']\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('Analisi Risultati Training', fontsize=16)\n",
        "\n",
        "    # Loss curves\n",
        "    axes[0,0].plot(history['epochs'], history['train_loss'], label='Train Loss', color='blue')\n",
        "    axes[0,0].plot(history['epochs'], history['val_loss'], label='Val Loss', color='red')\n",
        "    axes[0,0].set_title('Training & Validation Loss')\n",
        "    axes[0,0].set_xlabel('Epoch')\n",
        "    axes[0,0].set_ylabel('Loss')\n",
        "    axes[0,0].legend()\n",
        "    axes[0,0].grid(True)\n",
        "\n",
        "    # Accuracy curves\n",
        "    axes[0,1].plot(history['epochs'], history['train_acc'], label='Train Acc', color='blue')\n",
        "    axes[0,1].plot(history['epochs'], history['val_acc'], label='Val Acc', color='red')\n",
        "    axes[0,1].set_title('Training & Validation Accuracy')\n",
        "    axes[0,1].set_xlabel('Epoch')\n",
        "    axes[0,1].set_ylabel('Accuracy')\n",
        "    axes[0,1].legend()\n",
        "    axes[0,1].grid(True)\n",
        "\n",
        "    # Learning rate\n",
        "    if 'learning_rates' in history:\n",
        "        axes[1,0].plot(history['epochs'], history['learning_rates'], color='green')\n",
        "        axes[1,0].set_title('Learning Rate Schedule')\n",
        "        axes[1,0].set_xlabel('Epoch')\n",
        "        axes[1,0].set_ylabel('Learning Rate')\n",
        "        axes[1,0].set_yscale('log')\n",
        "        axes[1,0].grid(True)\n",
        "    else:\n",
        "        axes[1,0].text(0.5, 0.5, 'Learning rate data non disponibile',\n",
        "                       horizontalalignment='center', verticalalignment='center',\n",
        "                       transform=axes[1,0].transAxes)\n",
        "        axes[1,0].set_title('Learning Rate Schedule')\n",
        "\n",
        "    # Final metrics\n",
        "    if 'final_test_metrics' in results:\n",
        "        test_metrics = results['final_test_metrics']\n",
        "        metrics_names = list(test_metrics.keys())\n",
        "        metrics_values = list(test_metrics.values())\n",
        "        axes[1,1].bar(metrics_names, metrics_values, color='skyblue')\n",
        "        axes[1,1].set_title('Final Test Metrics')\n",
        "        axes[1,1].set_ylabel('Score')\n",
        "        axes[1,1].tick_params(axis='x', rotation=45)\n",
        "        axes[1,1].grid(axis='y')\n",
        "    else:\n",
        "        axes[1,1].text(0.5, 0.5, 'Metriche finali non disponibili',\n",
        "                       horizontalalignment='center', verticalalignment='center',\n",
        "                       transform=axes[1,1].transAxes)\n",
        "        axes[1,1].set_title('Final Test Metrics')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96]) # Aggiusta layout per suptitle\n",
        "    plt.show()\n",
        "\n",
        "    if 'final_test_metrics' in results:\n",
        "        print(\"üìä Risultati Finali Test:\")\n",
        "        for metric, value in results['final_test_metrics'].items():\n",
        "            print(f'  {metric}: {value:.4f}')\n",
        "else:\n",
        "    print(\"‚ùå Risultati del training non disponibili per l'analisi.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Informazioni modello finale\n",
        "if 'trainer' in locals() and hasattr(trainer, 'model') and 'results' in locals() and results:\n",
        "    model_info = trainer.model.get_model_info()\n",
        "\n",
        "    print(\"üß† Informazioni Modello:\")\n",
        "    print(f'  Parametri totali: {model_info[\"total_parameters\"]:,}')\n",
        "    print(f'  Parametri LSTM: {model_info[\"lstm_parameters\"]:,}')\n",
        "    print(f'  Parametri Shared Trunk: {model_info[\"shared_trunk_parameters\"]:,}')\n",
        "    print(f'  Input shape: {model_info[\"input_shape\"]}')\n",
        "    print(f'  Hidden size: {model_info[\"hidden_size\"]}')\n",
        "    print(f'  Num layers: {model_info[\"num_layers\"]}')\n",
        "\n",
        "    # Calcola dimensione modello in MB\n",
        "    model_size_mb = model_info['total_parameters'] * 4 / 1024 / 1024  # 4 bytes per parameter\n",
        "    print(f'  Dimensione modello: {model_size_mb:.1f} MB')\n",
        "else:\n",
        "    print(\"‚ùå Informazioni modello non disponibili (trainer o modello non inizializzati, o training non eseguito).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Step 5: Test Inference (Opzionale)\n",
        "\n",
        "Testiamo il modello addestrato con alcune predizioni di esempio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test rapido inference\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "if 'trainer' in locals() and hasattr(trainer, 'model'):\n",
        "    # Carica best model\n",
        "    best_model_path = os.path.join(trainer.model_dir, \"checkpoints/best_model.pth\")\n",
        "\n",
        "    if os.path.exists(best_model_path):\n",
        "        print(f\"üîÑ Caricamento best model da: {best_model_path}...\")\n",
        "        \n",
        "        try:\n",
        "            checkpoint = torch.load(best_model_path, map_location=trainer.device)\n",
        "            trainer.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            trainer.model.eval()\n",
        "            print(\"‚úÖ Modello caricato e impostato in modalit√† eval\")\n",
        "            \n",
        "            # Crea un batch di esempio per test\n",
        "            with torch.no_grad():\n",
        "                # Input di esempio (sostituire con dati reali)\n",
        "                batch_size = 4\n",
        "                sequence_length = trainer.config['data_loader']['sequence_length'] # Usa sequence_length da config\n",
        "                input_features = trainer.model.input_size\n",
        "                \n",
        "                sample_input = torch.randn(batch_size, sequence_length, input_features).to(trainer.device)\n",
        "                \n",
        "                print(f\"üîç Testing con input shape: {sample_input.shape}\")\n",
        "                \n",
        "                # Predizione\n",
        "                outputs = trainer.model(sample_input)\n",
        "                \n",
        "                print(\"üìä Output del modello:\")\n",
        "                for task_name, output in outputs.items():\n",
        "                    print(f\"  {task_name}: shape {output.shape}\")\n",
        "                    if 'classification' in task_name.lower() or 'pit_stop_prediction' in task_name.lower():\n",
        "                        probs = torch.softmax(output, dim=-1)\n",
        "                        predictions = torch.argmax(probs, dim=-1)\n",
        "                        print(f\"    Predictions: {predictions.cpu().numpy()}\")\n",
        "                        print(f\"    Max probabilities: {torch.max(probs, dim=-1)[0].cpu().numpy()}\")\n",
        "                    else: # Regression task\n",
        "                        print(f\"    Values (first 10): {output.cpu().numpy().flatten()[:10]}...\")\n",
        "                \n",
        "                print(\"‚úÖ Test inference completato con successo!\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Errore durante il caricamento del modello o l'inference: {e}\")\n",
        "            \n",
        "    else:\n",
        "        print(f\"‚ùå Best model non trovato in {best_model_path}. Completa prima il training.\")\n",
        "else:\n",
        "    print(\"‚ùå Trainer o modello non inizializzati. Impossibile eseguire l'inference.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Salva un riassunto finale\n",
        "import json\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "if 'results' in locals() and results and 'model_info' in locals() and model_info and 'dataset' in locals() and dataset is not None:\n",
        "    summary = {\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'dataset_info': {\n",
        "            'total_rows': len(dataset),\n",
        "            'total_columns': len(dataset.columns),\n",
        "            'years_covered': sorted(dataset['Year'].unique().tolist()) if 'Year' in dataset.columns else [],\n",
        "            'unique_drivers': dataset['Driver'].nunique() if 'Driver' in dataset.columns else 0,\n",
        "            'unique_events': dataset['EventName'].nunique() if 'EventName' in dataset.columns else 0\n",
        "        },\n",
        "        'training_results': results,\n",
        "        'model_info': model_info,\n",
        "        'gpu_info': {\n",
        "            'device_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU',\n",
        "            'memory_gb': torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n",
        "        }\n",
        "    }\n",
        "\n",
        "    summary_path = os.path.join(trainer.project_dir, \"training_summary.json\") # Salva in project_dir su Drive\n",
        "    try:\n",
        "        with open(summary_path, 'w') as f:\n",
        "            json.dump(summary, f, indent=2, default=str) # default=str per gestire tipi non serializzabili come numpy int64\n",
        "        print(f\"üìã Riassunto salvato in: {summary_path}\")\n",
        "        print(\"üéâ Training completo! Il modello √® pronto per l'uso.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Errore durante il salvataggio del riassunto: {e}\")\n",
        "else:\n",
        "    print(\"‚ùå Dati mancanti per salvare il riassunto (results, model_info, o dataset non disponibili).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test con dati specifici (pilota, anno, circuito)\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "print(\"üß™ Avvio test con dati specifici...\")\n",
        "\n",
        "# Parametri test\n",
        "driver = \"VER\"  # Sostituisci con il codice del pilota\n",
        "year = 2024      # Anno della gara\n",
        "circuit = \"Bahrain\"  # Nome del circuito (come nel file)\n",
        "\n",
        "# Esegui script test_model_new_data.py\n",
        "# Assicurati che lo script sia nella directory corretta o fornisci il path completo\n",
        "script_path = \"inference/test_model_new_data.py\" \n",
        "if not os.path.exists(script_path):\n",
        "    print(f\"‚ùå Script {script_path} non trovato. Verifica il path.\")\n",
        "else:\n",
        "    command = f\"python {script_path} --driver '{driver}' --year {year} --circuit '{circuit}'\"\n",
        "    print(f\"üîß Esecuzione comando: {command}\")\n",
        "    try:\n",
        "        process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "        stdout, stderr = process.communicate()\n",
        "\n",
        "        print(\"Output:\")\n",
        "        print(stdout)\n",
        "        if stderr:\n",
        "            print(\"Errori (se presenti):\")\n",
        "            print(stderr)\n",
        "        print(\"‚úÖ Test con dati specifici completato.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Errore durante l'esecuzione dello script di test: {e}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyN89oH7H7YqOQhLgPzXwJ3N",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
