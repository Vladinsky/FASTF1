{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# üèéÔ∏è Formula 1 Tire Change Prediction - Quick Start (Colab Pro)\n",
    "\n",
    "**Sistema completo di predizione cambi gomme F1 con RNN multi-task**\n",
    "\n",
    "Questo notebook ti guida attraverso il setup completo e il training del modello ottimizzato per Google Colab Pro.\n",
    "\n",
    "## üìã Pre-requisiti\n",
    "- Google Colab Pro (raccomandato)\n",
    "- Dati F1 caricati su Google Drive\n",
    "- GPU abilitata (Runtime > Change runtime type > GPU)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## üöÄ Step 1: Setup Ambiente\n",
    "\n",
    "Iniziamo con il setup automatico dell'ambiente ottimizzato per Colab Pro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Vladinsky/FASTF1.git\n",
    "%cd FASTF1/colab_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_env"
   },
   "outputs": [],
   "source": [
    "# Clona repository (se necessario) o carica files\n",
    "# NOTA: Se stai usando questo notebook, presumiamo che i file siano gi√† caricati\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Verifica che siamo nella directory corretta\n",
    "if not os.path.exists('colab_models'):\n",
    "    print(\"‚ùå Directory colab_models non trovata!\")\n",
    "    print(\"üîß Assicurati di aver caricato tutti i file nella directory corretta\")\n",
    "    print(\"üìÅ Struttura attesa:\")\n",
    "    print(\"   /content/colab_models/\")\n",
    "    print(\"   ‚îú‚îÄ‚îÄ setup_colab_pro.py\")\n",
    "    print(\"   ‚îú‚îÄ‚îÄ configs/\")\n",
    "    print(\"   ‚îú‚îÄ‚îÄ models/\")\n",
    "    print(\"   ‚îî‚îÄ‚îÄ ...\")\n",
    "else:\n",
    "    print(\"‚úÖ Directory colab_models trovata!\")\n",
    "    \n",
    "# Cambia directory\n",
    "%cd /content/colab_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_setup"
   },
   "outputs": [],
   "source": [
    "# Esegui setup automatico\n",
    "print(\"üöÄ Avvio setup automatico per Colab Pro...\")\n",
    "%run setup_colab_pro.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_section"
   },
   "source": [
    "## üìä Step 2: Unificazione Dati\n",
    "\n",
    "I dati F1 sono attualmente distribuiti in multiple cartelle. Li unifichiamo in un dataset completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_data"
   },
   "outputs": [],
   "source": [
    "# Verifica dati disponibili su Drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Drive se non gi√† fatto\n",
    "if not os.path.exists('/content/drive'):\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "# Check directories\n",
    "domenico_path = \"/content/drive/MyDrive/domenicoDL\"\n",
    "vincenzo_path = \"/content/drive/MyDrive/Vincenzo/processed_races\"\n",
    "\n",
    "print(\"üìÅ Controllo dati disponibili:\")\n",
    "print(f\"DomenicoDL: {os.path.exists(domenico_path)} - {len(os.listdir(domenico_path)) if os.path.exists(domenico_path) else 0} files\")\n",
    "print(f\"Vincenzo: {os.path.exists(vincenzo_path)} - {len(os.listdir(vincenzo_path)) if os.path.exists(vincenzo_path) else 0} files\")\n",
    "\n",
    "if not os.path.exists(domenico_path) and not os.path.exists(vincenzo_path):\n",
    "    print(\"‚ö†Ô∏è  Nessuna cartella dati trovata! Assicurati di aver caricato i dati su Drive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unify_data"
   },
   "outputs": [],
   "source": [
    "# Unifica tutti i dati in un dataset completo\n",
    "from data.data_unifier_complete import CompleteDataUnifier\n",
    "\n",
    "print(\"üîÑ Avvio unificazione dati...\")\n",
    "print(\"‚è±Ô∏è  Questo processo pu√≤ richiedere 10-15 minuti\")\n",
    "\n",
    "unifier = CompleteDataUnifier()\n",
    "dataset = unifier.unify_all_data()\n",
    "\n",
    "print(f'‚úÖ Unificazione completata!')\n",
    "print(f'üìä Dataset finale: {len(dataset):,} righe, {len(dataset.columns)} colonne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "explore_data"
   },
   "outputs": [],
   "source": [
    "# Esplorazione rapida del dataset unificato\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"üîç Esplorazione dataset unificato:\")\n",
    "print(f'Shape: {dataset.shape}')\n",
    "print(f'Anni: {sorted(dataset[\"Year\"].unique()) if \"Year\" in dataset.columns else \"N/A\"}')\n",
    "print(f'Piloti: {dataset[\"Driver\"].nunique() if \"Driver\" in dataset.columns else \"N/A\"}')\n",
    "print(f'GP: {dataset[\"EventName\"].nunique() if \"EventName\" in dataset.columns else \"N/A\"}')\n",
    "\n",
    "# Visualizzazione distribuzione anni\n",
    "if 'Year' in dataset.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    dataset['Year'].value_counts().sort_index().plot(kind='bar')\n",
    "    plt.title('Distribuzione Dati per Anno')\n",
    "    plt.xlabel('Anno')\n",
    "    plt.ylabel('Numero di Record')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_section"
   },
   "source": [
    "## üèãÔ∏è Step 3: Training del Modello\n",
    "\n",
    "Avviamo il training completo del modello LSTM multi-task ottimizzato per Colab Pro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# Verifica GPU e memoria\n",
    "import torch\n",
    "import psutil\n",
    "\n",
    "print(\"üîß Controllo risorse sistema:\")\n",
    "\n",
    "# GPU\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f'‚úÖ GPU: {gpu_name} ({gpu_memory:.1f}GB)')\n",
    "    \n",
    "    if \"T4\" in gpu_name or \"A100\" in gpu_name or \"V100\" in gpu_name:\n",
    "        print(\"üöÄ GPU ottima per training!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  GPU rilevata ma potrebbe essere lenta\")\n",
    "else:\n",
    "    print(\"‚ùå GPU non disponibile! Abilita GPU in Runtime > Change runtime type\")\n",
    "\n",
    "# RAM\n",
    "memory = psutil.virtual_memory()\n",
    "total_gb = memory.total / 1e9\n",
    "available_gb = memory.available / 1e9\n",
    "\n",
    "print(f'‚úÖ RAM: {total_gb:.1f}GB totale, {available_gb:.1f}GB disponibile')\n",
    "\n",
    "if total_gb > 20:\n",
    "    print(\"üöÄ RAM eccellente per Colab Pro!\")\n",
    "elif total_gb > 12:\n",
    "    print(\"‚úÖ RAM buona per training\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  RAM limitata, considera Colab Pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_trainer"
   },
   "outputs": [],
   "source": [
    "# Setup trainer\n",
    "from training.train_from_scratch_pro import ProTrainer\n",
    "\n",
    "print(\"üèãÔ∏è Inizializzazione trainer...\")\n",
    "\n",
    "# Crea trainer con configurazione ottimizzata\n",
    "trainer = ProTrainer(\n",
    "    config_path=\"configs/model_config_pro.yaml\",\n",
    "    project_dir=\"/content/drive/MyDrive/F1_TireChange_Project\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer inizializzato!\")\n",
    "print(f'üì± Device: {trainer.device}')\n",
    "print(f'üìÅ Model dir: {trainer.model_dir}')\n",
    "print(f'üìä Results dir: {trainer.results_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start_training",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# AVVIO TRAINING COMPLETO\n",
    "print(\"üöÄ AVVIO TRAINING DA ZERO...\")\n",
    "print(\"‚è±Ô∏è  Tempo stimato: 4-6 ore su Colab Pro\")\n",
    "print(\"üíæ Checkpoint automatici ogni 30 minuti\")\n",
    "print(\"üîÑ Puoi interrompere e riprendere il training\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Training con parametri ottimizzati\n",
    "results = trainer.train_complete(max_epochs=100)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üéâ TRAINING COMPLETATO!\")\n",
    "print(f'‚è±Ô∏è  Tempo totale: {results[\"total_training_time\"]/3600:.1f} ore')\n",
    "print(f'üéØ Best validation loss: {results[\"best_val_loss\"]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results_section"
   },
   "source": [
    "## üìä Step 4: Analisi Risultati\n",
    "\n",
    "Analizziamo i risultati del training e le performance del modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_training"
   },
   "outputs": [],
   "source": [
    "# Visualizza curve di training\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "history = results['training_history']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss curves\n",
    "axes[0,0].plot(history['epochs'], history['train_loss'], label='Train Loss', color='blue')\n",
    "axes[0,0].plot(history['epochs'], history['val_loss'], label='Val Loss', color='red')\n",
    "axes[0,0].set_title('Training & Validation Loss')\n",
    "axes[0,0].set_xlabel('Epoch')\n",
    "axes[0,0].set_ylabel('Loss')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "axes[0,1].plot(history['epochs'], history['train_acc'], label='Train Acc', color='blue')\n",
    "axes[0,1].plot(history['epochs'], history['val_acc'], label='Val Acc', color='red')\n",
    "axes[0,1].set_title('Training & Validation Accuracy')\n",
    "axes[0,1].set_xlabel('Epoch')\n",
    "axes[0,1].set_ylabel('Accuracy')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True)\n",
    "\n",
    "# Learning rate\n",
    "axes[1,0].plot(history['epochs'], history['learning_rates'], color='green')\n",
    "axes[1,0].set_title('Learning Rate Schedule')\n",
    "axes[1,0].set_xlabel('Epoch')\n",
    "axes[1,0].set_ylabel('Learning Rate')\n",
    "axes[1,0].set_yscale('log')\n",
    "axes[1,0].grid(True)\n",
    "\n",
    "# Final metrics\n",
    "test_metrics = results['final_test_metrics']\n",
    "metrics_names = list(test_metrics.keys())\n",
    "metrics_values = list(test_metrics.values())\n",
    "\n",
    "axes[1,1].bar(metrics_names, metrics_values, color='skyblue')\n",
    "axes[1,1].set_title('Final Test Metrics')\n",
    "axes[1,1].set_ylabel('Score')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Risultati Finali:\")\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f'  {metric}: {value:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_info"
   },
   "outputs": [],
   "source": [
    "# Informazioni modello finale\n",
    "model_info = trainer.model.get_model_info()\n",
    "\n",
    "print(\"üß† Informazioni Modello:\")\n",
    "print(f'  Parametri totali: {model_info[\"total_parameters\"]:,}')\n",
    "print(f'  Parametri LSTM: {model_info[\"lstm_parameters\"]:,}')\n",
    "print(f'  Parametri Shared Trunk: {model_info[\"shared_trunk_parameters\"]:,}')\n",
    "print(f'  Input shape: {model_info[\"input_shape\"]}')\n",
    "print(f'  Hidden size: {model_info[\"hidden_size\"]}')\n",
    "print(f'  Num layers: {model_info[\"num_layers\"]}')\n",
    "\n",
    "# Calcola dimensione modello in MB\n",
    "model_size_mb = model_info['total_parameters'] * 4 / 1024 / 1024  # 4 bytes per parameter\n",
    "print(f'  Dimensione modello: {model_size_mb:.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inference_section"
   },
   "source": [
    "## üöÄ Step 5: Test Inference (Opzionale)\n",
    "\n",
    "Testiamo il modello addestrato con alcune predizioni di esempio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_inference"
   },
   "outputs": [],
   "source": [
    "# Test rapido inference\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Carica best model\n",
    "best_model_path = \"/content/drive/MyDrive/F1_TireChange_Project/models/checkpoints/best_model.pth\"\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    print(\"üîÑ Caricamento best model...\")\n",
    "    \n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    trainer.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    trainer.model.eval()\n",
    "    \n",
    "    print(\"‚úÖ Modello caricato e impostato in modalit√† eval\")\n",
    "    \n",
    "    # Crea un batch di esempio per test\n",
    "    with torch.no_grad():\n",
    "        # Input di esempio (sostituire con dati reali)\n",
    "        batch_size = 4\n",
    "        sequence_length = 50\n",
    "        input_features = trainer.model.input_size\n",
    "        \n",
    "        sample_input = torch.randn(batch_size, sequence_length, input_features).to(trainer.device)\n",
    "        \n",
    "        print(f\"üîç Testing con input shape: {sample_input.shape}\")\n",
    "        \n",
    "        # Predizione\n",
    "        outputs = trainer.model(sample_input)\n",
    "        \n",
    "        print(\"üìä Output del modello:\")\n",
    "        for task_name, output in outputs.items():\n",
    "            print(f\"  {task_name}: shape {output.shape}\")\n",
    "            if 'classification' in task_name.lower():\n",
    "                probs = torch.softmax(output, dim=-1)\n",
    "                predictions = torch.argmax(probs, dim=-1)\n",
    "                print(f\"    Predictions: {predictions.cpu().numpy()}\")\n",
    "                print(f\"    Max probabilities: {torch.max(probs, dim=-1)[0].cpu().numpy()}\")\n",
    "            else:\n",
    "                print(f\"    Values: {output.cpu().numpy().flatten()[:10]}...\")  # primi 10 valori\n",
    "        \n",
    "        print(\"‚úÖ Test inference completato con successo!\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Best model non trovato. Completa prima il training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_summary"
   },
   "outputs": [],
   "source": [
    "# Salva un riassunto finale\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'dataset_info': {\n",
    "        'total_rows': len(dataset),\n",
    "        'total_columns': len(dataset.columns),\n",
    "        'years_covered': sorted(dataset['Year'].unique().tolist()) if 'Year' in dataset.columns else [],\n",
    "        'unique_drivers': dataset['Driver'].nunique() if 'Driver' in dataset.columns else 0,\n",
    "        'unique_events': dataset['EventName'].nunique() if 'EventName' in dataset.columns else 0\n",
    "    },\n",
    "    'training_results': results,\n",
    "    'model_info': model_info,\n",
    "    'gpu_info': {\n",
    "        'device_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU',\n",
    "        'memory_gb': torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_path = \"/content/drive/MyDrive/F1_TireChange_Project/training_summary.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"üìã Riassunto salvato in: {summary_path}\")\n",
    "print(\"üéâ Training completo! Il modello √® pronto per l'uso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test con dati specifici (pilota, anno, circuito)\n",
    "import subprocess\n",
    "\n",
    "print(\"üß™ Avvio test con dati specifici...\")\n",
    "\n",
    "# Parametri test\n",
    "driver = \"VER\"  # Sostituisci con il codice del pilota\n",
    "year = 2024      # Anno della gara\n",
    "circuit = \"Bahrain\"  # Nome del circuito (come nel file)\n",
    "\n",
    "# Esegui script test_model_new_data.py\n",
    "command = f\"python inference/test_model_new_data.py --driver '{driver}' --year {year} --circuit '{circuit}'\"\n",
    "process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "stdout, stderr = process.communicate()\n",
    "\n",
    "# Stampa output\n",
    "print(\"Output:\")\n",
    "print(stdout.decode())\n",
    "print(\"Errori (se presenti):\")\n",
    "print(stderr.decode())\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
